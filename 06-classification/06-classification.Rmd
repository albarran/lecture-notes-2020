---
title: "06 - Classification"
subtitle: "ml4econ, HUJI 2020"
author: "Itamar Caspi"
date: "April 26, 2020 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/middlebury.css", "style/middlebury-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "https://platform.twitter.com/widgets.js"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = FALSE,
  dev = "svglite",
  fig.ext = ".svg")

htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```


# Packages and setup

Use the [{`pacman`}](https://cran.r-project.org/web/packages/pacman/vignettes/Introduction_to_pacman.html) package that automatically loads and installs packages if necessary:

```{r pacman, message=FALSE, warning=FALSE, eval=TRUE}
if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse,   # for data wrangling and visualization
  tidymodels,
  knitr,       # for displaying nice tables
  here,        # for referencing folders and files
  glmnet,      # for estimating lasso and ridge
  ggmosaic,    # for tidy mosaic plots
)
```

Set a theme for `ggplot` (Relevant only for the presentation)
```{r ggplot_theme}
theme_set(theme_grey(20))
```

And set a seed for replication
```{r}
set.seed(1203)
```


---
# Outline

 - [Binary Classification Problems](#bin)
 
 - [The Confusion Matrix](#con)  
 
 - [The Logistic Regression Model](#logit)
 
 - [Sensitivity Specificity Trade-off](#sens)
 
 - [Multiclass classification (next time)](#mult)
 
 


---
class: title-slide-section-blue, center, middle
name: bin

# Binary Classification Problems


---
# Bill Gates on Testing for COVID-19

<midd-blockquote>"Basically, there are two critical cases: anyone who is symptomatic, and anyone who has been in contact with someone who tested positive. Ideally both groups would be sent a test they can do at home without going into a medical center. Tests would still be available in medical centers, but the simplest is to have the majority done at home. __To make this work, a government would have to have a website that you go to and enter your circumstances, including your symptoms. You would get a priority ranking, and all of the test providers would be required to make sure they are providing quick results to the highest priority levels.__ Depending on how accurately symptoms predict infections, how many people test positive, and how many contacts a person typically has, you can figure out how much capacity is needed to handle these critical cases. For now, most countries will use all of their testing capacity for these cases."    - Bill Gates.</midd-blockquote>


Source: ["The first modern pandemic by Bill Gates"](https://www.gatesnotes.com/Health/Pandemic-Innovation?WT.mc_id=20200423060000_Pandemic-Innovation_MED-media_&WT.tsrc=MEDmedia)

---
# Binary classification

Let $y_i$ denote the outcome of a COVID-19 test, where

$$y_{i}=\left\{\begin{array}{ll}
1 & \text { if positive, } \\
0 & \text { if negative, }
\end{array}\right.$$
where the values 1 and 0 are chosen for simplicity.<sup>1</sup>


Two types of questions we might ask:

1. What is the probability of being positive?
2. Can we classify an individual as positive/negative?


.footnote[
[*] It is common to find a $\{1, -1\}$ notation for binary outcomes in the ML literature.
]

---
# Israeli COVID-19 tests data

The [The Isreali Ministry of Health](https://data.gov.il/dataset/covid-19/resource/d337959a-020a-4ed3-84f7-fca182292308) provides information on more than 100,000 COVID-19 test results. Our aim here is to predict which person will be classified as "positive", i.e. infected by the virus, based on his symptoms and characteristics.

Outcome variable: `corona_result`

Features:

- Symptoms
  - `cough`
  - `fever`
  - `sore_throat`
  - `shortness_of_breath`
  - `head_ache`
- Characteristics
  - `age_60_and_above`
  - `gender`


---
# Read and examine the data

```{r}
covid_raw <- here("06-classification/data","covid_proc.csv") %>% 
  read_csv()
```


```{r}
covid_raw %>% glimpse()
```


Note that since $n=107,542$ and $p=7$, we should not worry much about overfitting. 

---
# Preprocessing

We'll now define all variables, outcome and features, as factors:
```{r}
covid <- covid_raw %>% 
  mutate_all(as_factor)
```

and extract the outcome and features as matrices (for later use with `glmnet`):
```{r}
x <- covid %>% 
  select(-corona_result) %>%
  model.matrix(~ .-1, data = .) 

y <- covid %>% pull(corona_result) %>% as_factor()
```

---
# Raw detection frequencies

How are test results distributed?
```{r}
covid %>% 
  group_by(corona_result) %>%
  count()
```


This is an example of __class imbalance__ (the distribution of examples across the known classes is skewed), which is a typical feature of classification problems.

---
# Measuring classification accuracy

What does MSE mean in the context of classification problems?

$$MSE = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 =\frac{1}{n}\sum_{i=1}^n \boldsymbol{1}_{\{y_i\neq \hat{y}_i\}}$$

In words: In this case, MSE measures the __missclassifcation rate__, i.e., the ratio between the number of missclassifications and the total number of observations.

__Classification accuracy__ is the total number of correct predictions divided by the total number of predictions made for a dataset.

Clearly, 
$$accuracy = 1 - missclasification.$$

Are missclasification/accuracy rates useful? Think imbalanced outcome.


---
# A naive classifier

Our naive "model" says: "classify everyone as being negative"
```{r}
covid %>% 
  mutate(corona_result = as_factor(corona_result)) %>% 
  mutate(.fitted_class = factor("negative", levels = c("negative", "positive"))) %>%
  conf_mat(corona_result, .fitted_class)
```


The accuracy of the model is $98,586/107,542 = 91.67\%$!

Pretty impressive! Or is it?

This naive classifier lacks the ability to discern one class versus the other, and more importantly, it fails to identify infected individuals - the thing we really care about!


---
class: title-slide-section-blue, center, middle
name: con

# The Confusion Matrix


---
# Beyond accuracy – other measures of performance

The __confusion matrix__ is a table that categorizes predictions according to whether they match the ground truth.

|                |          | __Truth__        |  __Truth__       |
|----------------|----------|------------------|------------------|
|                |          | Negative         | Positive         |
| __Prediction__ | Negative | _True negative_ (TN)  | _False negative_  (FN)|
| __Prediction__ | Positive | _False positive_  (FP) | _True positive_ (TP)   |
|                |          |                  |                   |

Note that $TP+TN+FP+TP=N$, where $N$ is the number of observations. Accuracy in this case is defined as $(TN + TP)/N$.

__Note:__ The confusion matrix can be extended to multiclass outcomes.


---
# Types of classification errors

__False positive rate:__ The fraction of negative examples that are
classified as positive, $0/98,586 = 0\%$ in example.

__False negative rate:__ The fraction of positive examples that are
classified as negative, $8,956/8,956 = 100\%$ in example.

Can we do better?

---
# A perfect classifier

Here is a simple example. Let’s assume we have a sample of 100 test results, and exactly 20 of them are labeled "positive". If our classifier was perfect, the confusion matrix would look like this:

|                |          | __Truth__        |  __Truth__       |
|----------------|----------|------------------|------------------|
|                |          | Negative         | Positive         |
| __Prediction__ | Negative | 80               | 0                |
| __Prediction__ | Positive | 0                | 20               |
|                |          |                  |                  |

That is, our classifier has a 100% accuracy rate, zero false positive and zero false negative.


---
# The realistic classifier

Now, here is a classifier that makes some errors:

|                |          | __Truth__        |  __Truth__       |
|----------------|----------|------------------|------------------|
|                |          | Negative         | Positive         |
| __Prediction__ | Negative | 70               | 10               |
| __Prediction__ | Positive | 5                | 15               |
|                |          |                  |                  |

In this example, 10 persons with the pathogen were classified as Negative (not infected), and 5 persons without the pathogen were classified as Positive (infected).



---
class: title-slide-section-blue, center, middle
name: logit

# Logistic Regession Model


---
# First things first: the linear probability model

Consider a dependent variable $y_{i} \in\{0,1\}$. Given a vector of features $\mathbf{x}_{i},$ the goal is to predict $\operatorname{Pr}\left(y_{i}=1 | \mathbf{x}_{i}\right)$.

Let $p_i$ denote the probability of seeing $y_i=1$ given $\mathbf{x}_i$, i.e., 
$$p_i\equiv\operatorname{Pr}\left(y_{i}=1 | \mathbf{x}_{i}\right)$$

The linear probability model specifies that
$$p_i=\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}$$

However, an OLS regression of $y_{i}$ on $\mathbf{x}_{i}$ ignores the discreteness of the dependent variable and does not constrain predicted probabilities to be between zero and one.



---
# Logitic regression model

A more appropriate model is the __logit model__ or __logistic regression model__ specifies as

$$p_i=\Lambda(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta})=\frac{\exp \left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)}{1+\exp \left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)}$$
where $\Lambda(\cdot)$ is the logistic cdf. As such, the model imposes the restriction that $0 \leq p_i \leq 1$.


---
# Odds-ratio

Note that
$$\frac{p_i}{1-p_i}=\exp \left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)$$
Taking logs yields
$$\ln\left(\frac{p_i}{1-p_i}\right) = \mathbf{x}_{i}^{\prime} \boldsymbol{\beta}$$
The above is useful representation of the logistic regression model. The LHS is called the log __odds ratio__ (or relative risk.)

Hence, we can say that the logistic regression model is linear in log odds-ratio.


---
# The likelihood function

__Likelihood__ refers to the probability of seeing the data given parameters.

$$\begin{aligned}
\mathrm{Likelihood} &=\prod_{i=1} \operatorname{Pr}\left(y_{i} | \mathbf{x}_{i}\right)\\
&=\prod_{i=1} p_{i}^{y_{i}}\left(1-p_{i}\right)^{1-y_{i}} \\
&=\prod_{i=1}^{n}\left(\frac{\exp \left(\mathbf{x}_{i}^{\prime} \beta\right)}{1+\exp \left(\mathbf{x}_{i}^{\prime} \beta\right)}\right)^{y_{i}}\left(\frac{1}{1+\exp \left(\mathbf{x}_{i}^{\prime} \beta\right)}\right)^{1-y_{i}}
\end{aligned}$$
taking (natural) logs yields the __log likelihood__
$$\log(\mathrm{Likelihood})=\sum_{i=1}^{N} \left[\log \left(1+e^{\left(\beta_{0}+x_{i}^{\prime} \beta\right)}\right)-y_{i} \cdot\left(\beta_{0}+x_{i}^{\prime} \beta\right)\right]$$
In estimation, we want to make the above as big as possible (hence, maximum likelihood estimation, MLE).

---
# Deviance

Another usefule conceppt is the __deviance__, a generalization of the concept of "least squares" to general linear models (such as logit), and is a measure of the distance between data and fit.

The relationship between deviance and likelihood is given by
$$\mathrm{Devience} = -2\times \log(\mathrm{Likelihood})+ \mathrm{Constant}$$
The constant wrapps terms that relate to the likelihood of the "perfect" model and we can mostly ignore it.

---
# Deviance and estimation

In estimation, we want to make deviance as _small_ as possible.
$$\begin{aligned}\mathrm{Deviance}&=-2\sum_{i=1}^{N} \left[\log \left(1+e^{\left(\beta_{0}+x_{i}^{\prime} \beta\right)}\right)-y_{i} \cdot\left(\beta_{0}+x_{i}^{\prime} \beta\right)\right]+ \mathrm{Constant}\\&\propto \sum_{i=1}^{N} \left[\log \left(1+e^{\left(\beta_{0}+x_{i}^{\prime} \beta\right)}\right)-y_{i} \cdot\left(\beta_{0}+x_{i}^{\prime} \beta\right)\right]\end{aligned}$$
This is the what R's `glm` function minimizes for logistic regressions.

(__NOTE:__ In linear models, the deviance is porportional to the RSS)


---
# Penalized logistic regression

We can also minimized the deviance subject to a standard lasso type ( $\ell_1$ norm) penalty on $\beta$:

$$\min _{\left(\beta_{0}, \beta\right) \in \mathbb{R}^{p+1}} \left[\frac{1}{N} \sum_{i=1}^{N} \log \left(1+e^{\left(\beta_{0}+x_{i}^{\prime} \beta\right)}\right)-y_{i} \cdot\left(\beta_{0}+x_{i}^{\prime} \beta\right)\right]+\lambda \|\beta\|_{1}$$

where again, the penalty is on the sum of the absolute values of $\beta$ (no including the intercept.)


---
# Back to the data: can we do better than being "naive"?

.pull-left[
There is some evidence that having fever is associated with being "positive".
```{r fever, fig.show='hide'}
covid %>% 
  ggplot() +
  geom_mosaic(
    aes(x = product(corona_result, fever),
        fill = corona_result)
  ) + 
  labs(
    x = "Fever",
    y = "Result",
    fill = ""
  )
```
]
.pull-right[
```{r, ref.label = 'fever', echo=FALSE}

```
]

---
# Back to the data: can we do better than being "naive"?

.pull-left[
and some evidence for an association with age (above 60)
```{r age, fig.show='hide'}
covid %>% 
  ggplot() +
  geom_mosaic(
    aes(x = product(corona_result, age_60_and_above),
        fill = corona_result)
  ) + 
  labs(
    x = "Above 60 years old",
    y = "Result",
    fill = ""
  )
```
]
.pull-right[
```{r, ref.label = 'age', echo=FALSE}

```
]

---
# Estimating the model using R

We will estimate the model using base R's `glm` (stands for generalized linear model) function:
```{r}
logit_model <- glm(
  corona_result ~ .,
  data = covid,
  family = "binomial"
)
```

Alternatively, we can estimate the regularized version of the model using `glmnet` with `family = "binomial"`:
```{r, eval=FALSE}
reg_logit_fit <- cv.glmnet(x, y, family = "binomial")
```

__SPOILER ALERT:__ `cv.glmnet` selects all features.

---
# Model output

The `tidy()` and `glance()` functions from the `{broom}` package provides tidy summary of the output from `glm` objects:
```{r}
logit_model %>% tidy()
```

```{r}
logit_model %>% glance()
```

---
# Model predictions (in sample)

.pull-left[
The figure on the right shows the resulting in-sample fit. There appears to be little overlap between probabilities for the true positives and the true negatives.
```{r box, fig.show='hide'}
covid_pred %>% 
  ggplot(aes(x = corona_result,
             y = .fitted,
             fill = corona_result)) +
  geom_boxplot() +
  labs(
    x = "Truth",
    y = "Prediction (y hat)",
    fill = ""
  )
```
]
.pull-right[
```{r, ref.label = 'box', echo=FALSE}

```
]

---
class: title-slide-section-blue, center, middle
name: sens

# Sensitivity Specifisity Trade-off

---
# Classification rule

To classify individuals as positive/negative we first need to set a __classification rule__ (cut-off), i.e., a probability $p^*$ above which we classify an individual as positive.  

For illustration, we'll set $p^*=0.8$:
```{r}
class_rule <- 0.8
```
This means that whenever $\hat{y}_i >0.8$, we would classify individual $i$ as `positive`.

__QUESTION:__ Is this rule overly aggressive or passive?

---
# Classification under the rule

```{r}
covid_pred <- logit_model %>% 
  augment(type.predict = "response") %>% 
  mutate(                                                                 
    .fitted_class = if_else(.fitted < class_rule, "negative", "positive"), #<<
    .fitted_class = as_factor(.fitted_class)                              
  ) %>%                                                                    
  select(corona_result, .fitted, .fitted_class)

covid_pred
```

---
# Sensitivity specificity trade-off

As we've seen, classifying everyone as "negative" $(p^*=1)$, fails to be specific, i.e., it fails to identify any positive results (what we really care about!):


__Sensitivity:__ The fraction of positive examples that are
classified as positive ("true positive rate"), $98,586/98,586 = 100\%$ in example.

__Specificity:__ The fraction of negative examples (`Yes`) that are
classified as negative ("true negative rate"), $0/8,956 = 0\%$ in example.


Note that in general, 
$$\text{false negative rate} = 1 - \text{specificity}$$
$$\text{false positive rate} = 1 - \text{sensitivity}$$


---
# Our model's confusion matrix

.pull-left[
The function `cnf_mat()` from the `{yardstick}` package provides easy access to a model's confusion matrix and the implied performance statistics.
```{r}
covid_conf_mat <- 
  covid_pred %>% 
  conf_mat(corona_result, .fitted_class) 

covid_conf_mat
```
]
.pull-right[
```{r}
covid_conf_mat%>% 
  summary() %>% 
  filter(.metric %in% c("accuracy", "sens", "spec")) %>% 
  mutate("1-.estimate" = 1 - .estimate)
```
As we can see, for `class_rule = 0.8`, the model is highly sensitive but not so sensitive. Clearly, changing the rule would change the model's classification properties.
]


---
# Visualizing the sens-spec trade-off with ROC curves

.pull-left[
A receiver __operating characteristic (ROC) curve__, plots sensitivity against 1-specificity. By doing so, it highlights the trade-off between false-positive and true-positive error rates as the classifier threshold is varied.
]
.pull-right[
```{r, echo=FALSE, fig.align='center'}
include_graphics("figs/roc.png")
```
Source: ["Machine Learning with R: Expert techniques for predictive modeling"](https://www.amazon.com/Machine-Learning-techniques-predictive-modeling/dp/1784393908) 
]


---
# Our model's ROC curve

.pull-left[
On the left, you can see our model's ROC curve, plotted using the `roc_curve()` function. The red and blue dots correspond to two cut-offs, 0.8 and 0.2, respectively.
```{r roc_curve, fig.height=2, fig.show='hide'}
covid_pred %>% 
  roc_curve(corona_result, .fitted) %>% 
  autoplot() +
  geom_point(
    aes(x = 0.690, y = 0.999),
    color = "blue"
  ) + # 0.8 threshold
  geom_point(
    aes(x = 0.436, y = 0.950),
    color = "red"
  ) # 0.2 threshold
```
Note that we've used `.fitted` instead of `.fitted_class`.
]
.pull-right[
```{r, ref.label = 'roc_curve', echo=FALSE}

```
]

---
# Area under the curve (AUC)

.pull-left[
- Ranking of classifiers can be made based on the area under the ROC curve (AUC).
- For example, a perfect classifier has `auc=1` and a classifier with no discriminate value has `auc=0.5`.
- Nevertheless, identical `auc` values can result from two different ROC curves. Thus, qualitative examination is warrant.
```{r}
covid_pred %>% roc_auc(corona_result, .fitted)
```

]
.pull-right[
```{r, echo=FALSE, fig.align='center'}
include_graphics("figs/auc.png")
```
Source: ["Machine Learning with R: Expert techniques for predictive modeling"](https://www.amazon.com/Machine-Learning-techniques-predictive-modeling/dp/1784393908) 
]


---
# AUC anc cross-validation

When it comes to classification tasks, it is sometimes more reasonable to tune the penalty parameter based on classification performance metrics (and not on, say, deviance.)

For example, we can use the `cv.glmnet()` function while setting the `type.measure = "auc"` in order to tune based on auc values
```{r, eval=FALSE}
cvfit <- cv.glmnet(
  x, y, 
  family = "binomial",
  type.measure = "auc"  #<<
)
```
or set `type.measure = "class"` to tune based on the misclassification rate.


---
class: title-slide-section-blue, center, middle
name: mult

# Multiclass Classification
## (Next time)



---
class: .title-slide-final, center, inverse, middle

# `slides::end()`

[<i class="fa fa-github"></i> Source code](https://github.com/ml4econ/notes-spring2019/tree/master/06-classification)  




---
# References

Lantz, Brett. Machine Learning with R: Expert techniques for predictive modeling, 3rd Edition (p. 333). Packt Publishing.